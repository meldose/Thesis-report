\documentclass[12pt]{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=3cm, right=2cm]{geometry}
\usepackage{babel}
\usepackage{float}
\usepackage{ragged2e}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{lscape}
\graphicspath{ {figures/} }
\usepackage{array}
\usepackage{eso-pic,xcolor}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{array}
\usepackage{wrapfig}





\newcommand\AtPageUpperRight[1]{\AtPageUpperLeft{
   \makebox[\paperwidth][r]{#1}}}

\begin{titlepage}

\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=8cm]{Figures/th-deggendorf.png} % Centered image
    \vspace{1cm} % Space after the image
    
    \textbf{Master Thesis}{\normalfont\fontsize{14}{16}\bfseries}
    
    \vspace{0.7cm}
    
    \mbox{Deggendorf Institute of Technology, Deggendorf}
    
    \vspace{0.5cm}
    \mbox{Faculty of Mechanical and Mechatronics Engineering}
    
    \vspace{0.5 cm}
    
    \mbox{Master Mechatronics and Cyberphysical Systems}
    
    \vspace{4.0 cm}
    
    {Verfolgung eines Förderbands mit zwei Robotern mithilfe einer Bildverarbeitungskamera}
    
    \vspace{0.5 cm}
    
    \textbf{Tracking a conveyor belt with robot using a machine vision camera}
    
    \vspace{0.5 cm}
    
    \mbox{Master Thesis to obtain Academic Degree}
    
    \vspace{0.5cm}
    
    \textbf{Master of Engineering(M.Eng)}
    
    \vspace{1.5 cm}
    
    \mbox{submitted by: Midhun Eldose, 22101196}
    
    \vspace{0.5 cm}
    
    \mbox{first examiner: Prof. Ginu Paul Alunkal}
    
    \vspace{1.5 cm}
    
    \mbox{Deggendorf, 30.04.2025}

\end{center}
\end{titlepage}

\begin{titlepage}
 \makebox[\paperwidth][r]{#1}


\begin{center}

\vspace{0.5cm}

  \textbf{Confidential Disclosure Agreement}{\normalfont\fontsize{14}{16}\bfseries}
   
   \vspace{0.5 cm}
    
   \mbox{between}
   
   \vspace{0.5 cm}
   
   \textbf{Deggendorf Institute of Technology}{\normalfont\fontsize{14}{16}\bfseries}
   
   \vspace{0.2 cm}
   
   \mbox{Campus Deggendorf}
   
   \vspace{0.2 cm}
   
   \mbox{Dieter-Görlitz-Platz 1, }
   
    \vspace{0.2 cm}
    
   \mbox{94469, Deggendorf}
   
   \vspace{2.5 cm}
    
   \mbox{Faculty of Mechanical Engineering and Mechatronics}
    \vspace{0.2 cm}
    \mbox{Major: Mechatronics and Cyberphysical Systems}

    \vspace{0.5 cm}

    \mbox{ Prof. Ginu Paul Alunkal}

    \vspace{0.5 cm}

     \mbox{ (in the following "Deggendorf Institute of Technology")}

     \vspace{0.5 cm}
     
     \mbox{ and}

     \vspace{0.5 cm}

     \mbox{Midhun Eldose}
       \vspace{0.5 cm}

     \mbox{ (in the following "NEURA Robotics GmbH") }
     
     \vspace{0.5 cm}

     \mbox{ (in the following singularly and jointly "Contractual Partner") }

\end{center}
      
      \vspace{1.5 cm}
      
     \raggedright
     \textbf{Preamble}

    
    \vspace{0.5 cm}
     
     The Deggendorf Institute of Technology supervises an examination paper with the topic of\textbf{ Tracking a conveyor belt with two robots using a machine vision camera}

     \vspace{0.5cm}
     
     \mbox{}

     \vspace{0.5cm}
\begin{center}
    (in the following "examination paper"), in which, among other things, confidential 
     information of the company is processed. Simultaneously, confidential information 
    also shared with the company in the context of supervision by the Deggendorf Institute of Technology.
\end{center}

\end{titlepage}

\newpage
\linespread{1.5}
\raggedright
\textbf{Declaration}

\vspace{0.5cm}

\mbox{Name of the Student: Midhun Eldose}

\vspace{0.5cm}

\mbox{Name of the first Examiner:  Prof. Ginu Paul Alunkal }

\vspace{1 cm}

\mbox{Title of master thesis:}

\vspace{0.5 cm}

Tracking a conveyor belt with two robots using a machine vision camera
\vspace{1.5 cm}

 I hereby declare that I have written this thesis independently. I have not submitted it for any other examination purposes. I have not used other references or material than mentioned in the bibliography and I have marked all literal analogous citations.

 \vspace{1.5 cm}
\raggedright
Deggendorf,30.04.2025
\hspace{4 cm}
Signature of the student:


\linespread{1.5}
\pagenumbering{roman}
\newpage

\begin{document}

\newpage
\tableofcontents
\newpage
\listoffigures
\addcontentsline{toc}{section}{List of Figures}
\newpage
\listoftables
\addcontentsline{toc}{section}{List of Tables}
\newpage
\raggedright
\newpage
\raggedright
\addcontentsline{toc}{section}{Acknowledgement}
\begin{center}
    \textbf{Acknowledgement}
\end{center}
    \raggedright

    I am highly indebted to NEURA Robotics GmbH, Metzingen for their guidance and constant supervision as well as for providing necessary information and resources for the Master thesis and for the support in completing the report

    \vspace{1cm}
    
    I express my special gratitude to Sugeeth Gopinathan, Head of Software Strategy, Mr. Phoung Nguyen Software Application Expert , Dr. Norman Kohler and Mr. Florian Schnös for instructing, providing information about how tasks are to be done and the flow of work, and guiding me for the thesis. Besides that, I also thank all the members for their guidance and keen support at various stages of my thesis.

    \vspace{1cm}

    I am very thankful to my Academic guide Prof. Ginu Paul Alunkal Firsching for his full support and encouragement. I owe him for his timely guidance, suggestions, and very constructive criticism which contributed immensely to the evolution of my thesis.
    
    \vspace{1.5 cm}
    
    \raggedleft
    Midhun Eldose


\newpage

\begin{abstract}{\normalfont\fontsize{14}{16}\bfseries}
\raggedright

Conveyor tracking using collaborative robots equipped with vision cameras represents a significant advancement in modern production. This innovative approach enables cobots to interact dynamically with moving goods on conveyor belts, thereby enhancing efficiency, precision, and adaptability. By utilizing vision cameras, cobots can accurately detect, locate, and track objects in real-time, accommodating changes in shape, speed, and orientation. The integration of vision technology allows cobots to perform tasks such as sorting, assembly, and pick-and-place operations with minimal human intervention. \\

This thesis explores the technical aspects of conveyor tracking with cobots, highlighting the role of vision systems in improving automation capabilities, reducing error rates, and enabling flexible manufacturing environments. It also addresses the challenges of synchronizing cobot actions with fast-moving conveyors and discusses the potential benefits for industries such as logistics, packaging, and electronics assembly. \\

The experimental work involves establishing a collaborative environment of cobots for conveyor tracking experiments using a vision camera. The primary objective is to enhance the tracking accuracy of objects and capture various shapes on the conveyor belt. The experimental setup includes two cobots, a conveyor belt, two grippers, and objects of different shapes. As the conveyor belt moves these objects, the vision camera captures their images and poses, along with their part coordinate system (PCS). This information is relayed to the first robot, which sorts the objects based on shape. The second robot then synchronizes with the sorted objects in a designated capture zone, picking them up and placing them into user-defined target boxes. Once filled, the boxes are removed via the conveyor or another transport mechanism.
}
\newpage
\chapter{\section{\mbox{Introduction}}{\normalfont\fontsize{14}{16}\bfseries}}
\label{introduction}

\begin{justify}
Conveyor tracking is a vital component of robot manipulators in industrial robotic applications. The task grows more complex as the target moves down the production line. Conveyor tracking is utilizing a robot to track and retrieve an article from a conveyor belt. Robots require object information, such as position, orientation, velocity, size, etc., to catch items. Applications for using vision sensors to recognize things include the military, medical, biology, engineering, education, and factory operations [1]. When a robot is assembling electric parts into a product, it needs to be able to track and reorganize objects on the conveyor belt and correct position errors. The vision sensor can interpret this information rapidly.\\

A robot's job in robotic conveyor tracking is to follow and retrieve goods from a conveyor belt. Robots require information about an object, like its position, orientation, velocity, size, and other characteristics, before they may take it from an automation line. The information that vision sensors can supply about an object on the conveyor belts is more than that of ultrasonic and infrared ray sensors. An object tracking system typically consists of the following steps: capturing an image, identifying objects, and retrieving data regarding the position and orientation of the objects. A conveyor system's item tracking procedure needs to be quick enough to accommodate a real-time setting. This article describes a tracking system for robots that makes use of vision data that is taken from consecutive frames of images. 


\subsection{Photoneo Camera}
\begin{justify}
The main Vision camera we have used to track the moving object's motion through the conveyor belt is a Photoneocamera (MotionCam 3D M+), which has advanced settings that can capture and detect the object's position. The photoneo camera mainly consists of 3D sensing technology, which contains parallel structured light that helps provide the light source to detect the objects. Using this camera, the camera can capture accurate point clouds and a standard intensity image of the object.
Its foundation is a specialized CMOS image sensor that uses Photoneo's patented Parallel Structured Light technology.\\\\

\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=5cm]{Thesis-report/Figures/camera.jpeg} % Centered image
    \vspace{1cm} % Space after the image
\end{center}


The camera's carbon fiber body is lightweight and guarantees the same degree of stiffness as scanners. Three parts comprise the 3D camera: a camera unit with our proprietary Mosaic Shutter CMOS image sensor, a laser projection unit, and a processor unit with a GPU that acts as the brains behind intelligent applications. A sequential structured light, which is utilized in numerous meteorological applications, is the primary technological driver in the first group. One well-suited representative of this category is the 3D scanner range from Photoneo. This technique is not appropriate for dynamic scenes because it uses sequential (multi-frame) capturing. This section is the source of the parallel structured light.

\subsubsection{Time fo Flight}
The time it takes for light to travel from an illumination source to a scene and back is measured by time-of-flight. The speed of light itself is the primary obstacle in this situation. Usually, the phase shift of a modulated signal is used to measure time. High pixel modulation frequencies must be used in order to achieve a respectable level of depth accuracy. The primary drawback in this case is physics since a greater frequency results in less charge transfer, which lowers contrast and SNR. The restriction clearly suggests an accuracy level that TOF systems can achieve. Usually, it falls within the centimeter range. Another issue is interreflections, which can significantly bend the surface.

\subsubsection{Active stereo}
By projecting a synthetic texture onto an object, active stereo addresses the unreliable passive stereo. Nevertheless, it still has to resolve the computationally demanding picture correspondence matching problem. Given the complexity of the matching problem, the projected texture can be either high frequency, which can satisfy a higher resolution but usually has poor reliability, or low frequency, which typically uses random laser dots and can offer higher reliability but poor resolution (the features are sparse).

\subsubsection{Structured patterns/dots}
A spatially encoded pattern is used in structured patterns/dots technology to encode depth disparity information into pattern patches, which are usually projected through a laser diffraction grating in the form of a carefully planned dot collection. The camera must be able to see enough of the patch to reassemble the coding and accurately record the depth information. This produces artifacts on surfaces' edges and tiny objects. The Nyquist-Shannon theorem requires an order of magnitude higher camera resolution to reconstruct individual dots in the projection (and hence 3D measurements). Modern systems use about 25 camera pixels for each 3D measurement, producing about 70k 3D points.

\subsubsection{Parallel Structured Light}
Parallel Structured Light parallelizes the sequential structured light using a sophisticated sensor design, which enables it to record the scene illuminated by various patterns simultaneously. In addition to sharing many of the sequential structured light's benefits, such as resolution and accuracy, it also overcomes one of its main drawbacks: the inability to capture a dynamic environment. By simultaneously projecting and capturing numerous encoded patterns, Photoneo's Parallel Structured Light gets around the restriction. Pixel modulations within our proprietary CMOS sensor are used to accomplish this. Multiple groups of separately modified pixels make up the sensor itself. A control unit that operates in tandem with the projection is in charge of these groups. The coded patterns are inserted into the groups rather than changing the projection itself. The sensor may generate over 20 distinct virtual representations of the scene illuminated by the coded patterns injected at the end of the frame. The method is universal and may be modified on the fly to accommodate various materials and light sources by using any type of pattern typically used for sequential structured light.










\newpage

\chapter{\section{\mbox{Literature Review }}{{\normalfont\fontsize{14}{16}\bfseries}}
\label{Literature}

\newpage

\chapter{\section{\mbox{Methodology }}{{\normalfont\fontsize{14}{16}\bfseries}}
\label{Literature}
\subsection{Software Codes}
\subsubsection{Servo-X}
\subsubsection{Servo-J}

\newpage

\chapter{\section{\mbox{Working Principle }}{{\normalfont\fontsize{14}{16}\bfseries}}

\subsubsection{Photoneo Camera}
The main Vision camera we have used to track the moving object's motion through the conveyor belt is a Photoneocamera (MotionCam 3D M+), which has advanced settings that can capture and detect the object's position. The photoneo camera mainly consists of 3D sensing technology, which contains parallel structured light that helps provide the light source to detect the objects. Using this camera, the camera can capture accurate point clouds and a standard intensity image of the object. \\\


\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=5cm]{Thesis-report/Figures/camera.jpeg} % Centered image
    \vspace{1cm} % Space after the image
\end{center}

Its foundation is a specialized CMOS image sensor that uses Photoneo's patented Parallel Structured Light technology. The camera's carbon fiber body is lightweight and guarantees the same degree of stiffness as scanners. Three parts comprise the 3D camera: a camera unit with our proprietary Mosaic Shutter CMOS image sensor, a laser projection unit, and a processor unit with a GPU that acts as the brains behind intelligent applications. A sequential structured light, which is utilized in numerous meteorological applications, is the primary technological driver in the first group. One well-suited representative of this category is the 3D scanner range from Photoneo. This technique is not appropriate for dynamic scenes because it uses sequential (multi-frame) capturing. This section is the source of the parallel structured light.\\\\



\subsubsection{6-Axis Cobot-Delta}
The cobot that we have used for the conveyor tracking system is a 6-axis cobot called a Delta robot. The robot mainly moves towards its target position by getting the values from the camera in the form of quaternion values.\\\\
\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=5cm]{Thesis-report/Figures/robot.jpeg} % Centered image
    \vspace{1cm} % Space after the image
\end{center}
\subsubsection{VGC10-Gripper}
The OnRobot VGC10 electrical compact vacuum gripper offers unlimited customization and suction cup options, making it suitable for tight environments and lifting small, odd-shaped, and heavy objects. It features two independently controlled air channels, increasing efficiency and reducing cycle time.\\\\
\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=5cm]{Thesis-report/Figures/gripper.jpeg} % Centered image
    \vspace{1cm} % Space after the image
\end{center}
With no compressor or air supply, it's easy to move and redeploy. Once the robot gets near the object, the suction gripper is activated, thereby providing suction to the object and helping to pick up the object and place it in the target box.


\subsubsection{Objects for picking}
The objects that we have used for conveyor tracking include a trapezoid, pipe socket, and circle ball.\\
\subsubsection{Socket-pipe} The socket pipe is: \\\\
\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=5cm]{Thesis-report/Figures/pipe.png} % Centered image
    \vspace{1cm} % Space after the image
\end{center}
\subsubsection{Trapezoid}
The trapezoid  is: \\\\

\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=5cm]{Thesis-report/Figures/trapezoid.png} % Centered image
    \vspace{1cm} % Space after the image
\end{center}
\subsubsection{Conveyor Belt}
The conveyor belt is the main belt where we have to place the object, thereby putting the object into dynamic mode.\\


\label{Literature}

\subsection{Photoneo Camera-Web Interface}

\newpage

\chapter{\section{\mbox{Results and Discussions }}{{\normalfont\fontsize{14}{16}\bfseries}}
\label{Literature}

\newpage

\chapter{\section{\mbox{Conclusions and Future Prospects}}{{\normalfont\fontsize{14}{16}\bfseries}}
\label{Literature}
\subsection{Conclusion}


Conveyor tracking systems have become indispensable components in modern industrial and logistical operations, facilitating the seamless movement of goods across various stages of production, warehousing, and distribution. Over the years, advancements in technology have significantly enhanced the efficiency, accuracy, and reliability of these systems. Key achievements in conveyor tracking include:

\begin{itemize}

\item Enhanced Automation and Integration: The integration of conveyor tracking with other automated systems, such as Warehouse Management Systems (WMS) and Enterprise Resource Planning (ERP) software, has streamlined operations, reduced manual intervention, and minimized errors.

 \item Real-Time Monitoring and Data Analytics: Modern conveyor tracking systems leverage sensors, IoT devices, and advanced analytics to provide real-time insights into the movement of goods. This capability enables proactive decision-making, predictive maintenance, and optimized resource allocation.

\item Improved Accuracy and Efficiency: Innovations in tracking technologies, including barcode scanners, RFID tags, and machine vision, have significantly improved the accuracy of item identification and tracking, leading to reduced loss, theft, and misplacement of goods.

\item Scalability and Flexibility: Contemporary conveyor systems are designed to be highly scalable and adaptable, allowing businesses to adjust their operations based on fluctuating demand and changing market conditions without substantial infrastructure overhauls.

\item Sustainability and Energy Efficiency: Advances in conveyor technology have also focused on energy efficiency and sustainability, incorporating energy-saving motors, regenerative braking systems, and eco-friendly materials to reduce the environmental footprint of conveyor operations.
\end{itemize}


\newpage

\subsubsection{Future Scope }

The future of conveyor tracking is poised to be shaped by continued technological innovations and evolving industry demands. Potential developments and areas for growth include:

\begin{itemize}

\item Artificial Intelligence and Machine Learning: Integrating AI and ML algorithms can enhance predictive maintenance, optimize routing, and improve overall system intelligence. These technologies can analyze vast amounts of data to identify patterns, predict failures, and recommend operational adjustments in real-time.

\item Advanced IoT Integration: The proliferation of IoT devices will further enhance the connectivity and interoperability of conveyor tracking systems. Enhanced IoT integration will enable more granular monitoring, improved data collection, and seamless communication between disparate systems and devices.

\item Augmented Reality (AR) and Virtual Reality (VR): AR and VR technologies can revolutionize training, maintenance, and operational planning. For instance, maintenance personnel can use AR glasses to receive real-time guidance and overlays while servicing conveyor systems, reducing downtime and improving efficiency.

\item Blockchain for Enhanced Security and Transparency: Implementing blockchain technology can provide immutable records of product movement and handling, enhancing traceability, security, and transparency throughout the supply chain.

\item Robotic Integration and Automation: The integration of robotics with conveyor tracking systems can further automate material handling processes. Collaborative robots (cobots) can work alongside conveyor systems to perform tasks such as sorting, packaging, and inspection with greater precision and speed.

\item Sustainability and Green Technologies: Future conveyor systems will increasingly incorporate sustainable practices, such as the use of renewable energy sources, recyclable materials, and energy-efficient designs. Innovations aimed at reducing waste and minimizing the environmental impact will become paramount.

\item Enhanced User Interfaces and Human-Machine Interaction: The development of more intuitive and user-friendly interfaces will improve the interaction between operators and conveyor tracking systems. Voice-controlled interfaces, touchless controls, and advanced visualization tools will make system management more accessible and efficient.

\item Edge Computing and Enhanced Processing Capabilities: Implementing edge computing will allow data processing to occur closer to the source, reducing latency and improving the responsiveness of conveyor tracking systems. This will be particularly beneficial for real-time applications and environments with limited connectivity.

\item Customization and Modular Designs: Future conveyor tracking systems will offer greater customization and modularity, allowing businesses to tailor systems to their specific needs without extensive redesigns. Modular components can be easily added, removed, or reconfigured to adapt to changing operational requirements.

\item Enhanced Cybersecurity Measures: As conveyor tracking systems become more connected and reliant on digital technologies, ensuring robust cybersecurity will be critical. Future developments will focus on implementing advanced security protocols, encryption methods, and intrusion detection systems to protect against cyber threats.
\end{itemize}

\newpage

\chapter{\section{\mbox{References}}{{\normalfont\fontsize{14}{16}\bfseries}}
\label{Literature}



\end{document}


\addcontentsline{toc}{section}{Abstract} 


\pagenumbering{arabic}
\setcounter{page}{1}
\raggedright
\input{Chapters/Introduction}
\newpage
\input{Chapters/Literature Review }
\newpage
\input{Chapters/Methodology}
\newpage
\input{Chapters/Result}
\newpage
\input{Chapters/Conclusion and Future Prospects}
\newpage
\addcontentsline{toc}{section}{References}
\input{Chapters/Reference}
\newpage
\input{Chapters/Annex}





 

























