\documentclass[12pt]{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=3cm, right=2cm]{geometry}
\usepackage{babel}
\usepackage{float}
\usepackage{ragged2e}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{lscape}
\graphicspath{ {figures/} }
\usepackage{array}
\usepackage{eso-pic,xcolor}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{array}
\usepackage{wrapfig}





\newcommand\AtPageUpperRight[1]{\AtPageUpperLeft{
   \makebox[\paperwidth][r]{#1}}}

\begin{titlepage}

\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=8cm]{Figures/th-deggendorf.png} % Centered image
    \vspace{1cm} % Space after the image
    
    \textbf{Master Thesis}{\normalfont\fontsize{14}{16}\bfseries}
    
    \vspace{0.7cm}
    
    \mbox{Deggendorf Institute of Technology, Deggendorf}
    
    \vspace{0.5cm}
    \mbox{Faculty of Mechanical and Mechatronics Engineering}
    
    \vspace{0.5 cm}
    
    \mbox{Master Mechatronics and Cyberphysical Systems}
    
    \vspace{4.0 cm}
    
    {Verfolgung eines Förderbands mit zwei Robotern mithilfe einer Bildverarbeitungskamera}
    
    \vspace{0.5 cm}
    
    \textbf{Tracking a conveyor belt with robot using a machine vision camera}
    
    \vspace{0.5 cm}
    
    \mbox{Master Thesis to obtain Academic Degree}
    
    \vspace{0.5cm}
    
    \textbf{Master of Engineering(M.Eng)}
    
    \vspace{1.5 cm}
    
    \mbox{submitted by: Midhun Eldose, 22101196}
    
    \vspace{0.5 cm}
    
    \mbox{first examiner: Prof. Ginu Paul Alunkal}
    
    \vspace{1.5 cm}
    
    \mbox{Deggendorf, 30.04.2025}

\end{center}
\end{titlepage}

\begin{titlepage}
 \makebox[\paperwidth][r]{#1}


\begin{center}

\vspace{0.5cm}

  \textbf{Confidential Disclosure Agreement}{\normalfont\fontsize{14}{16}\bfseries}
   
   \vspace{0.5 cm}
    
   \mbox{between}
   
   \vspace{0.5 cm}
   
   \textbf{Deggendorf Institute of Technology}{\normalfont\fontsize{14}{16}\bfseries}
   
   \vspace{0.2 cm}
   
   \mbox{Campus Deggendorf}
   
   \vspace{0.2 cm}
   
   \mbox{Dieter-Görlitz-Platz 1, }
   
    \vspace{0.2 cm}
    
   \mbox{94469, Deggendorf}
   
   \vspace{2.5 cm}
    
   \mbox{Faculty of Mechanical Engineering and Mechatronics}
    \vspace{0.2 cm}
    \mbox{Major: Mechatronics and Cyberphysical Systems}

    \vspace{0.5 cm}

    \mbox{ Prof. Ginu Paul Alunkal}

    \vspace{0.5 cm}

     \mbox{ (in the following "Deggendorf Institute of Technology")}

     \vspace{0.5 cm}
     
     \mbox{ and}

     \vspace{0.5 cm}

     \mbox{Midhun Eldose}
       \vspace{0.5 cm}

     \mbox{ (in the following "NEURA Robotics GmbH") }
     
     \vspace{0.5 cm}

     \mbox{ (in the following singularly and jointly "Contractual Partner") }

\end{center}
      
      \vspace{1.5 cm}
      
     \raggedright
     \textbf{Preamble}

    
    \vspace{0.5 cm}
     
     The Deggendorf Institute of Technology supervises an examination paper with the topic of\textbf{ Tracking a conveyor belt with robot using a machine vision camera}

     \vspace{0.5cm}
     
     \mbox{}

     \vspace{0.5cm}
\begin{center}
    (in the following "examination paper"), in which, among other things, confidential 
     information of the company is processed. Simultaneously, confidential information 
    also shared with the company in the context of supervision by the Deggendorf Institute of Technology.
\end{center}

\end{titlepage}

\newpage
\linespread{1.5}
\raggedright
\textbf{Declaration}

\vspace{0.5cm}

\mbox{Name of the Student: Midhun Eldose}

\vspace{0.5cm}

\mbox{Name of the first Examiner:  Prof. Ginu Paul Alunkal }

\vspace{1 cm}

\mbox{Title of master thesis:}

\vspace{0.5 cm}

Tracking a conveyor belt with two robots using a machine vision camera
\vspace{1.5 cm}

 I hereby declare that I have written this thesis independently. I have not submitted it for any other examination purposes. I have not used other references or material than mentioned in the bibliography and I have marked all literal analogous citations.

 \vspace{1.5 cm}
\raggedright
Deggendorf,30.04.2025
\hspace{4 cm}
Signature of the student:


\linespread{1.5}
\pagenumbering{roman}
\newpage

\begin{document}

\newpage
\tableofcontents
\newpage
\listoffigures
\addcontentsline{toc}{section}{List of Figures}
\newpage
\listoftables
\addcontentsline{toc}{section}{List of Tables}
\newpage
\raggedright
\newpage
\raggedright
\addcontentsline{toc}{section}{Acknowledgement}
\begin{center}
    \textbf{Acknowledgement}
\end{center}
    \raggedright

    I am highly indebted to NEURA Robotics GmbH, Metzingen for their guidance and constant supervision as well as for providing necessary information and resources for the Master thesis and for the support in completing the report

    \vspace{1cm}
    
    I express my special gratitude to Sugeeth Gopinathan, Head of Software Strategy, Mr. Phoung Nguyen Software Application Expert , Dr. Norman Kohler and Mr. Florian Schnös for instructing, providing information about how tasks are to be done and the flow of work, and guiding me for the thesis. Besides that, I also thank all the members for their guidance and keen support at various stages of my thesis.

    \vspace{1cm}

    I am very thankful to my Academic guide Prof. Ginu Paul Alunkal Firsching for his full support and encouragement. I owe him for his timely guidance, suggestions, and very constructive criticism which contributed immensely to the evolution of my thesis.
    
    \vspace{1.5 cm}
    
    \raggedleft
    Midhun Eldose


\newpage

\begin{abstract}{\normalfont\fontsize{14}{16}\bfseries}
\raggedright

Conveyor tracking using collaborative robots equipped with vision cameras represents a significant advancement in modern production. This innovative approach enables cobots to interact dynamically with moving goods on conveyor belts, thereby enhancing efficiency, precision, and adaptability. By utilizing vision cameras, cobots can accurately detect, locate, and track objects in real time, accommodating changes in shape, speed, and orientation. The integration of vision technology allows robots to perform tasks such as sorting, assembly, and pick-and-place operations with minimal human intervention. This thesis explores the technical aspects of conveyor tracking with cobots, highlighting the role of vision systems in improving automation capabilities, reducing error rates, and enabling flexible manufacturing environments. It also addresses the challenges of synchronizing cobot actions with fast-moving conveyors and discusses the potential benefits for industries such as logistics, packaging, and electronics assembly. The experimental work involves establishing a collaborative environment of cobots for conveyor tracking experiments using a vision camera. The primary objective is to enhance the tracking accuracy of objects and capture various shapes on the conveyor belt. The experimental setup includes a cobot, a conveyor belt, one gripper, and objects of different shapes. As the conveyor belt moves these objects, the vision camera captures their images and poses, along with their part coordinate system (PCS). The robot then synchronizes with the objects in a designated capture zone, picking them up and placing them into user-defined target boxes. 

\newpage
\chapter{\section{\mbox{Introduction}}{\normalfont\fontsize{14}{16}\bfseries}}
\label{introduction}

\begin{justify}
Conveyor tracking is a vital component of robot manipulators in industrial robotic applications. The task grows more complex as the target moves down the production line. Conveyor tracking is utilizing a robot to track and retrieve an article from a conveyor belt. Robots require object information, such as position, orientation, velocity, size, etc., to catch items. Applications for using vision sensors to recognize things include the military, medical, biology, engineering, education, and factory operations [1]. When a robot is assembling electric parts into a product, it needs to be able to track and reorganize objects on the conveyor belt and correct position errors. The vision sensor can interpret this information rapidly.\\

A robot's job in robotic conveyor tracking is to follow and retrieve goods from a conveyor belt. Robots require information about an object, like its position, orientation, velocity, size, and other characteristics, before they may take it from an automation line. The information that vision sensors can supply about an object on the conveyor belts is more than that of ultrasonic and infrared ray sensors. An object tracking system typically consists of the following steps: capturing an image, identifying objects, and retrieving data regarding the position and orientation of the objects. A conveyor system's item tracking procedure needs to be quick enough to accommodate a real-time setting. This article describes a tracking system for robots that makes use of vision data that is taken from consecutive frames of images. 


\subsection{Photoneo Camera}
\begin{justify}
The main Vision camera we have used to track the moving object's motion through the conveyor belt is a Photoneocamera (MotionCam 3D M+), which has advanced settings that can capture and detect the object's position. The photoneo camera mainly consists of 3D sensing technology, which contains parallel structured light that helps provide the light source to detect the objects. Using this camera, the camera can capture accurate point clouds and a standard intensity image of the object.
Its foundation is a specialized CMOS image sensor that uses Photoneo's patented Parallel Structured Light technology.\\\\

\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=5cm]{Thesis-report/Figures/camera.jpeg} % Centered image
    \vspace{1cm} % Space after the image
\end{center}


The camera's carbon fiber body is lightweight and guarantees the same degree of stiffness as scanners. Three parts comprise the 3D camera: a camera unit with our proprietary Mosaic Shutter CMOS image sensor, a laser projection unit, and a processor unit with a GPU that acts as the brains behind intelligent applications. A sequential structured light, which is utilized in numerous meteorological applications, is the primary technological driver in the first group. One well-suited representative of this category is the 3D scanner range from Photoneo. This technique is not appropriate for dynamic scenes because it uses sequential (multi-frame) capturing. This section is the source of the parallel structured light.

\subsubsection{Time fo Flight}
The time it takes for light to travel from an illumination source to a scene and back is measured by time-of-flight. The speed of light itself is the primary obstacle in this situation. Usually, the phase shift of a modulated signal is used to measure time. High pixel modulation frequencies must be used in order to achieve a respectable level of depth accuracy. The primary drawback in this case is physics since a greater frequency results in less charge transfer, which lowers contrast and SNR. The restriction clearly suggests an accuracy level that TOF systems can achieve. Usually, it falls within the centimeter range. Another issue is interreflections, which can significantly bend the surface.

\subsubsection{Active stereo}
By projecting a synthetic texture onto an object, active stereo addresses the unreliable passive stereo. Nevertheless, it still has to resolve the computationally demanding picture correspondence matching problem. Given the complexity of the matching problem, the projected texture can be either high frequency, which can satisfy a higher resolution but usually has poor reliability, or low frequency, which typically uses random laser dots and can offer higher reliability but poor resolution (the features are sparse).

\subsubsection{Structured patterns/dots}
A spatially encoded pattern is used in structured patterns/dots technology to encode depth disparity information into pattern patches, which are usually projected through a laser diffraction grating in the form of a carefully planned dot collection. The camera must be able to see enough of the patch to reassemble the coding and accurately record the depth information. This produces artifacts on surfaces' edges and tiny objects. The Nyquist-Shannon theorem requires an order of magnitude higher camera resolution to reconstruct individual dots in the projection (and hence 3D measurements). Modern systems use about 25 camera pixels for each 3D measurement, producing about 70k 3D points.

\subsubsection{Parallel Structured Light}
Parallel Structured Light parallelizes the sequential structured light using a sophisticated sensor design, which enables it to record the scene illuminated by various patterns simultaneously. In addition to sharing many of the sequential structured light's benefits, such as resolution and accuracy, it also overcomes one of its main drawbacks: the inability to capture a dynamic environment. By simultaneously projecting and capturing numerous encoded patterns, Photoneo's Parallel Structured Light gets around the restriction. Pixel modulations within our proprietary CMOS sensor are used to accomplish this. Multiple groups of separately modified pixels make up the sensor itself. \\\\ 
\newpage A control unit that operates in tandem with the projection is in charge of these groups. The coded patterns are inserted into the groups rather than changing the projection itself. The sensor may generate over 20 distinct virtual representations of the scene illuminated by the coded patterns injected at the end of the frame. The method is universal and may be modified on the fly to accommodate various materials and light sources by using any type of pattern typically used for sequential structured light.\\

\subsubsection{Vision Controller}\\
The figure below shows the vision controller connected to the photoneo camera and the robotic controller using Ethernet (IPV4 address). The vision controller is connected to a PC to get the Web interface to trigger the camera and see the Web interface for communication.Gigabit Ethernet cables (Cat5e or higher) are needed to connect the 3D sensor. A gigabit switch can be used to connect several 3D sensors. The Ethernet connection from the switch or 3D sensor is linked to the Scanner's physical network port in both scenarios.To visualize 3D scans, calibrate robot cameras, and establish localization settings, a well-connected connection to the 3D Sensor is required.
To set up the network interface that the vision controller uses to connect to the Photoneo 3D Sensor(s), go to the Network page and select the Scanner interface section.Both a fixed IPv6 link-local address and a programmable IPv4 address are features of Photoneo 3D Sensors.
IPv6 connections are better than IPv4 ones. However, the IPv4 address is used if the IPv6 connection is banned or fails.Consequently, having a legitimate IPv4 network configuration is advised.
The interface can be set up to operate a DHCP server or to utilize any random static IP address. Use the appropriate IP address settings on the sensor side through the PhoXi Control program in both situations.\\

\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=5cm]{Thesis-report/Figures/vision controller.png} % Centered image
    \vspace{1cm} % Space after the image
\end{center}\\
\subsubsection{Marker Pattern}\\
The figure below shows the Board that helps calibrate the conveyor belt, which aligns the coordinates of the object with the conveyor belt for objects passing through the belt.\\
\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=5cm]{Thesis-report/Figures/marker_pattern.png} % Centered image
    \vspace{1cm} % Space after the image
\end{center}\\

\newpage

\chapter{\section{\mbox{Literature Review }}{{\normalfont\fontsize{14}{16}\bfseries}}
\label{Literature}

\newpage

\chapter{\section{\mbox{Methodology }}{{\normalfont\fontsize{14}{16}\bfseries}}
\label{Literature}
\subsection{Robotic Movement}\\\\
Given a point in the world coordinate, the angle of each joint is calculated based on an inverse kinematics (IK) equation. In general, there may be no analytic IK solution from a manipulator for the configurations of each joint. The numerical method is introduced to solve the problem for
general manipulators. The velocity of the joint can be mapped to Cartesian space with the Jacobian linearization method.\\

\begin{document}

\[
v = J(q) \dot{q}
\]

Pseudo-inverse is used to solve the joint:\\
\begin{document}

\[
\dot{q} = J^\dagger \dot{x}, \quad J^\dagger = (J^T J)^{-1} J^T
\]

velocity for the linear velocity in Cartesian space. First, FK is used to determine the manipulator pose. Next, the IK solver uses a pseudo-inverse calculation to update the joint angles. Until the end tip reaches the desired posture with an acceptable error, these two stages are repeated. If the starting value and changing rate are appropriately calibrated, the dynamic gain causes the speed to reach its maximum and minimum quickly. Joint angles have an impact on the inaccuracy during the iteration process. Inappropriate angles cause the gain to drop dynamically, and vice versa.\\

\subsection{Position of Camera}\\
When mounting a camera for vision operation, it is best to keep it stable and as close to the robot's work envelope as feasible, without interfering with or restricting the movement of the robot arm. The longer the part must travel from the camera's location into the robot's reachable workspace, the more errors the encoder counts will produce since the accuracy of conveyor tracking with vision depends on the coordinate transformation between the camera's picture-taking location and the robot's work envelope (See Figure 4). The camera must be oriented downwards since the components are positioned on the conveyor belt's surface and vision recognition is done on the parts' upper surface. For the camera's 25 mm lens to cover the entire visible portion of the conveyor belt, a camera fixture was constructed to hold the camera directly above the upstream sensor. The upstream sensor is also used to activate the camera to take a photo when parts are presented at this point, so the picture-taking location is placed at the top of the upstream sensor. Following their placement in fixed locations, the camera and conveyor must be calibrated using the calibration program on the robot controller to configure the camera's properties and determine how the locations of the camera and conveyor relate to the robot's world coordinate.\\

\subsection{Object Identification}\\
The STL file is uploaded to the Web interface, thereby helping the camera to identify the object in the moving conveyor belt tracking system. Once the object is detected the camera helps in getting the object's position and its quaternion values.

\subsection{Trajectory Planning}\\
Using a set of predetermined points, trajectory planning aims to produce stable and fluid motion in world coordinates. A few characteristics, such as the maximum velocity, acceleration, and jerk, will be taken into account to demonstrate the continuous motion of the robot arm. The constant context of temporal movement is unique. Additionally, this function has a first derivative and a second derivative. In addition to increasing wear on the mechanism, jerky action frequently causes vibration in the robot manipulator.\\
The fundamental path interpolation functions, such as linear and point-to-point movement, are carried out when the precondition parameters have been addressed. Interpolation, which is determined by previous factors, will produce the path segments. A self-defined function is crucial to the creation of path segments in Cartesian space. Joint command and linear command can both follow the point-to-point movement. Every joint's present condition is indicated in the joint space.\\
\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=5cm]{Thesis-report/Figures/new.png} % Centered image
    \vspace{1cm} % Space after the image
\end{center}

All joints will arrive simultaneously before approaching targets since the velocity is zero at the target point.In contrast to point-to-point motion, linear movement at the end-effector makes it easy to demonstrate the condition from the reference coordinate. Fig. 6 displays the current position of the end point in the reference coordinate. The motion will only be linked to one DoF during variation if the current velocity point toward the target position is specified. To execute gripping operations, the robot arm will thereafter alter its motion via path segments.Manipulators can grab a stationary workpiece with the aid of trajectory planning and object recognition algorithms.

The fundamental path interpolation functions, such as linear and point-to-point movement, are carried out when the precondition parameters have been addressed. Interpolation, which is determined by previous factors, will produce the path segments. 
\subsection{Traking Method}\\
In the preceding part, the picking procedure was put into practice. With some dynamic adjustments, the situation is now more challenging. The robot should be updated with the object's information when it begins to move on the conveyor to use the tracing talent. The tracking technique presents certain difficulties when using a visual control system. First, we must use real-time frame-by-frame image processing to track the objects. Next, we use the average of the previous ten frames as a guide. Second, we must specify the gripper's position about the moving object. We came up with one method for determining the objects' positions. Kinects that can recognize objects can also provide us with depth information in addition to the target's color. Next, using the information gathered, we devise a method to obtain 3D coordinates and calibrate the scale to the y and z-axes while taking into account the depth frame, which is the x-axis under Kinect view. Finally, the robot's reaction will complete the grabbing maneuver by following the updated data. Our tracking strategy is divided into three subsections. By calibrating the y and z axes, we may create a prospective viewpoint to enhance object location after obtaining the past 10 frames as a reference to compare with the present frame. The tracker would ultimately follow the target by the predicted trajectory after we had the position of the tracker and the target.\\\\
Initially obtain the tracker's (gripper) and the target's (workpiece) instantaneous positions independently once the camera has determined the outlines of objects. First, the contour is the difference between the color frame and the reference and current frames. The coordinates of the items in the depth frame are then obtained. The situation along the x and z axes in the depth frame and the y and z axes from the Kinect color frame are among the desired data. \\

The tracker and target belong to different x-y planes; thus, we may utilize a threshold regarding the data along the z-axis in the color frame in Kinect view to separate the objects. It is referred to as the robot palm if the coordinate along the z-axis item is more than the threshold; if not, it is the industrial component. 
\subsection{Position Calibration}\\
Static pose recognition will end when the tracking approach is activated. The end-effector and object positions on the conveyor are among the data. The workpiece will proceed through the conveyor in a straight line. Therefore, only the workpieces' positions will be changed, and their orientation will remain unchanged. It is important to note that the end-effector might follow the subsequent instructions based on the tracking approach. This doesn't assist with grabbing articles, though. The gripper should take aim-off into account when doing object tracking to ensure that the entire gripping process is complete. Thus, proper gripping position by tracking method turns out to be the important point. Without additional indication, the arm might not be aware of some issues that arise on the conveyor. A webcam is set aside for the best gripping. The difference between frames will be recorded as the robot is instructed to begin tracking by the control center. Verify the gripper's direction first. The webcam then captures the image when the gripper is grasping. Finally, a version that includes the gripping stance and orientation will be saved. If the system detects the same object again, it will update the subsequent pose and orientation.\\
\subsection{Encoder offsets}\\
The main method for defining robot movements about a conveyor belt is to model the belt using a unique kind of position data called belt variables. The position of a reference frame fixed to the moving belt conveyor is defined by a belt variable, which is regarded as a relative transformation with a component variable in time. The relationship between a belt encoder and the position and speed of the reference frame, which keeps a fixed position and orientation concerning the belt, can be described using such a belt variable. A belt variable is defined as:\\

\[
\text{DEFBELT} \, \% \text{belt-variable} = \text{nominal-trans}, \, \text{scale factor}
\]
Where: scale factor is a calibrating constant that specifies the ratio between the elementary displacement of the belt and one pulse of the encoder; nominal-trans is the value in R6 of the (simple or composed) relative transformation that defines the position and orientation of the conveyor belt, and belt-variable is the name of the belt variable to be defined. The nominal transformation specifies a position (X, Y, Z) that points to the approximate center of the belt concerning the robot's base frame, the X axis of the nominal transformation indicates the direction of motion of the belt; and the XY plane defined by this transformation is parallel to the conveyor belt surface.\\
\[
\text{XYZ-instantaneous} =
\text{XYZ-nominal} + \text{belt-distance} \cdot \text{vers}(X_{l,\sim,\text{minal\_rrarrv}})
\]

\[
\text{with} \quad \text{belt-distance} =
(\text{encoder-count} - \text{encoder-offset}) \cdot \text{scale-factor}
\]
Where encoder-count is the encoder's read contents and encoder-offset is used to determine the belt's reference frame's instantaneous location (x,, y, ) about its nominal location (x,!, y, ). Specifically, a displacement carried out by the conveyor can be nullified by using the offset of the belt (by setting the offset value to the current value of the encoder's counter). The conveyor's visual robot tracking system takes into consideration fluctuating belt offsets, which are often altered by software operations.\\
\[
\text{SETBELT} \ \% \text{belt-variable} = \text{expression}
\]
For picking-on-the-fly robot tasks, a fast digital-input interrupt line that detects the occurrence of an external event of the type "an object has completely entered the Conveyor Belt Window" (and hence is completely visible) will initiate the dynamic robot-vision synchronization based on the "Look-and-Move" cooperation principle. A photocell detects this event and uses it to determine whether to take an image of the moving item.\\
\subsection{Moving Calibration through Conveyor belt}\\
The trajectory follows the instructions produced from trajectory planning and eye-in-hand vision each time the robot arm's palm begins to perform a moving task. The border of the conveyor will show up on the top side of the image when viewed through a camera. Because the conveyor's moving path is linear, the robot's motion can be corrected by following the conveyor's edge's slope. There are numerous image processing options available for searching a graph in a picture. The Hough transform, also referred to as object detection, is one easy technique. In camera view, the item is just a collection of colored points beneath the background. A specific point or high-dimensional plane will be mapped to those point sets. This can produce a parametric equation that represents every scenario that could be used to characterize the pattern's feature. Additionally, look for the extreme value to determine the pattern's location. Finally, group all of the spots that are comparable to show the workpiece feature.The line equation in polar coordinates to determine the conveyor's line edge. The polar parametric form has the advantage of having no singular condition in the domain of sinusoidal functions.\\

\begin{equation}
r = x\cos\theta + y\sin\theta
\end{equation}

\newpage
\subsection{Software Codes}
\subsubsection{Servo-X}
\subsubsection{Servo-J}
\subsubsection{Calibration for camera for Workspace}\\\\
The camera calibration is performed through an extrinsic method in which the camera is fixed in a position and the calibration ball is fixed at the end of the robot TCP.\\\\


\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=16cm]{Thesis-report/Figures/calibration.jpeg} % Centered image
    \vspace{1cm} % Space after the image
\end{center} \\
The above figure shows how the calibration is happening by adding the point by fixing the point where the robot Coordinates [X, Y, Z ]coordinates rotation values will save the points by saving the points (represented as green point). Here we need to add 9 points to get the calibration workspace. After the calibration, the average value from the 9 points is taken into consideration.\\

The below figure shows the calibration method used in this study (Extrinsic Calibration) (EC), where the camera is fixed at location and 9 points are taken into consideration where the average value is finally taken as the value for the calibration.\\
\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=11cm]{Thesis-report/Figures/extrinsic.png} % Centered image
    \vspace{1cm} % Space after the image
\end{center} 
This kind of calibration is employed in configurations where the 3D sensor is fixedly positioned within the robotic cell, often above the bin. The 3D sensor does not have to be stationary concerning the robotic cell itself; it just needs to be stationary concerning the robot's foundation. The calibration matrix will become invalid and the entire calibration process will need to be restarted if there is a change in the relative position between the 3D sensor and the robot after the calibration. In other words, after the system has been calibrated, the 3D sensor cannot be changed concerning the robot.The calibration matrix specifies the transformation straight to the robot base coordinate system because the 3D sensor is fixed.With this configuration, the robot's mobility is unrestricted, and it doesn't need to halt during the scan acquisition, unlike with the hand-eye approach.
\subsubsection{Calibration Ball}\\\\
A calibration ball serves as the calibration object. It is also feasible to use a bespoke ball with the right characteristics in place of the calibration ball that Photoneo provides; the ball needs to be exactly round and composed of a surface that is good for scanning (smooth and not extremely reflective). The ball needs to be attached to the gripper or the flange, which are the robot's endpoints. During the calibration process, make sure the ball is securely attached and stays in place. Verify that the ball can be seen clearly by the 3D sensor.\\\\

\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=5cm]{Thesis-report/Figures/ball.png} % Centered image
    \vspace{1cm} % Space after the image
\end{center} 


\newpage

\subsubsection{Calibration of Conveyor Belt}.

The conveyor belt calibration can be done as follows:\\

\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=16cm]{Thesis-report/Figures/conveyor_belt with cmaera.jpg} % Centered image
    \vspace{1cm} % Space after the image
\end{center} \\\\
The above figure consists of the Inputs and outputs, where inputs include:\
\begin{itemize}
\item Scan request\\
The Sacn request consists of a  Python code that is used to send a request to the camera, which is then taken by the vision controller or the vision control box that helps trigger the camera.
 \item Trigger scan\\
 The trigger scan can be of the SW or HW method, where the output is hardware or Software method.

\item Trigger \\\\
Initially, the scan request is given to the Vision controller, creating a trigger scan that helps the camera trigger. \\
The output section includes the: \\
\end{itemize}
\begin{itemize}
\item Coordinates of localized object\\
The coordinates of the localized object consist of the object [X, Y, Z, Rx, Ry, Rz, and W], where X, Y, and Z include the object coordinates and [Rx, Ry, Rz, and W] include the quaternion values (rotation values), which are identified by the vision camera.
\item Scan\\
Once the trigger commands reach the camera, the scan will start to take place and take the objects as mentioned above.
\item Final position \\ 
The final position is considered as the location where the object coordinates, along with the calibration offset and traveled distance.
\end{itemize}


1. \textbf{Calculate the pick pose:} \\\\
2. \textbf{Components of the pick pose calculation:} 
Coordinates from localization in marker space (calculate the transformation matrix from camera space to marker space to calibrate the marker pattern)

Possible triggers for the distance traveled from acquisition to the beginning of picking in conveyor tracking mode include:\\\\
The scan request function pho_wait_for_req_completion() in SW HW-HW output of the trigger\\\\

\textbf{Calibration Distance:}
The distance between the marker pattern positions for the camera calibration and custom workspace calibration (the linear transformation between the camera and the custom WS origin) is known as the calibration distance.

\textbf{Localized posture minus calibration distance plus traveled distance from the scan is the formula.} \\\\

\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=16cm]{Thesis-report/Figures/conveyor_belt.jpg} % Centered image
    \vspace{1cm} % Space after the image
\end{center}  \\\\

In the above figure, we can see the conveyor belt having the calibration board, that is used for the Calibration which determines the X, Y, and Z coordinates of the object along with the conveyor belt, where MSD means marker space displacement, TD means travel distance, and TSc means trigger to scan completion in meters where the target position is the T[X, Y, Z, Rx, Ry, Rz], which represents the quaternion value of the object that need to be converted into Euler values that are used to move the robot to that location. \\\\

\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=16cm]{Thesis-report/Figures/coordinates.png} % Centered image
    \vspace{1cm} % Space after the image
\end{center}  \\\\
In the above figure, the number represents the coordinates of the object after the scanning of the object done after the trigger. Once these quaternion values are attained, these values are then sent to the robotic controller for the robot to move to the required target position.\\\\
Calibration of the marker pattern from the camera to the conveyor belt, which is the common origin \\\\
1. Attach the appropriate calibration pattern to the conveyor belt. \\\\
2. Use PhoXi Control's marker pattern to save calibration \\\\
You can use it as a custom scanning profile in VS Space by adding a triggering item for the sensor at the marker pattern export photo's origin.
Robot to conveyor belt: The robot moves the conveyor belt from a shared origin, but the marker pattern stays in place according to the conveyor belt. This allows the robot to reach the marker pattern, calibrate the custom coordinate space in the robot track, and save the "calibration" encoder value, which synchronizes the robot and conveyor.




\newpage

\chapter{\section{\mbox{Working Principle }}{{\normalfont\fontsize{14}{16}\bfseries}}
\subsubsection{Steps for Conveyor Tracking Visual System:}\\
Two steps are necessary for conveyor visual tracking: \\
\begin{itemize}
\item Item detection\\
\item Object tracking.\\
\end{itemize}\\
There are several ways to identify items. Some potential methods include color information-based recognition, self-organizing maps, template matching, and the temporal difference of two consecutive image samples. Due to their slowness, self-organizing maps cannot function in real time. Template matching requires prior knowledge of object information in order to match objects. Unfortunately, because of the necessary computational load, this method cannot be used in real-time. The color information solution overcomes the first two limitations, however it is not compatible with binary images. The object recognition method that leverages the difference between two photographs can be useful when the environment does not change quickly over time.

\subsubsection{Photoneo Camera}
The main Vision camera we have used to track the moving object's motion through the conveyor belt is a Photoneocamera (MotionCam 3D M+), which has advanced settings that can capture and detect the object's position.\\\\ The photoneo camera mainly consists of 3D sensing technology, which contains parallel structured light that helps provide the light source to detect the objects. Using this camera, the camera can capture accurate point clouds and a standard intensity image of the object. \\\


\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=5cm]{Thesis-report/Figures/camera.jpeg} % Centered image
    \vspace{1cm} % Space after the image
\end{center}

Its foundation is a specialized CMOS image sensor that uses Photoneo's patented Parallel Structured Light technology. The camera's carbon fiber body is lightweight and guarantees the same degree of stiffness as scanners. Three parts comprise the 3D camera: a camera unit with our proprietary Mosaic Shutter CMOS image sensor, a laser projection unit, and a processor unit with a GPU that acts as the brains behind intelligent applications. A sequential structured light, which is utilized in numerous meteorological applications, is the primary technological driver in the first group. One well-suited representative of this category is the 3D scanner range from Photoneo. This technique is not appropriate for dynamic scenes because it uses sequential (multi-frame) capturing. This section is the source of the parallel structured light.\\

\subsubsection{Communication of Robot With Vision Camera}\\
The robot interface on the vision controller and the robot module operating on the robot controller enables communication between the vision controller and the robot controller.\\\\

\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=14cm]{Thesis-report/Figures/communication.png} % Centered image
    \vspace{1cm} % Space after the image
\end{center}\\
The Vision Controller's scanner port is directly attached to a single Photoneo 3D Sensor.
● The Vision Controller's network port is directly connected to a desktop PC for remote control.
● The Robot controller is directly attached to the Vision Controller's Robot port.\\\\
\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=10cm]{Thesis-report/Figures/connection.png} % Centered image
    \vspace{1cm} % Space after the image
\end{center}\\

 \subsubsection{Protocol TCP/IP}\\
The robot controller and the vision controller communicate via the TCP/IP protocol. Network connectivity is an optional feature that some robot controllers do not come with by default. All prerequisites for the full installation of the Robot module are listed in the Robot integration guides' Prerequisites section.\\

\subsubsection{Channel of communication}\\
The Robot module functions as a TCP client, while the vision controller establishes a TCP server. The client is called the Action Request Client, while the server is called the Action Request Server. The Action Request Client communicates with the Action Request Server over this channel. After receiving the action request, the vision controller carries it out and replies to the robotic controller.\\\\

\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=14cm]{Thesis-report/Figures/channel.png} % Centered image
    \vspace{1cm} % Space after the image
\end{center}\\
\subsubsection{Network (EC, HC) }\\
The robot interface (and robot controller IP) and the Vision controller's 3D sensor interface must be properly configured for the Vision controller to communicate with both the robot and the 3D sensor. This is not required for marker space calibration.\\
\subsubsection{Vision }\\
It is necessary to thoroughly configure the visual system that will be calibrated. Make sure the following vision system parameters are set up correctly before beginning the calibration:
● Scanner ID: This vision system uses a 3D sensor. The linked 3D sensors will appear in the drop-down list of available 3D sensors if the scanner interface is set up properly.
The calibration space and scanner position specify the 3D sensor's mount point and, consequently, the calibration type. The scanner model is automatically calculated based on the selected 3D sensor.\\
\subsubsection{6-Axis Cobot-Delta}
The cobot that we have used for the conveyor tracking system is a 6-axis cobot called a Delta robot. The robot mainly moves towards its target position by getting the values from the camera in the form of quaternion values.\\\\
\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=3cm]{Thesis-report/Figures/robot.jpeg} % Centered image
    \vspace{1cm} % Space after the image
\end{center}
\subsubsection{VGC10-Gripper}
The OnRobot VGC10 electrical compact vacuum gripper offers unlimited customization and suction cup options, making it suitable for tight environments and lifting small, odd-shaped, and heavy objects. It features two independently controlled air channels, increasing efficiency and reducing cycle time.\\\\
\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=5cm]{Thesis-report/Figures/gripper.jpeg} % Centered image
    \vspace{1cm} % Space after the image
\end{center}
With no compressor or air supply, it's easy to move and redeploy. Once the robot gets near the object, the suction gripper is activated, thereby providing suction to the object and helping to pick up the object and place it in the target box.\\\\

\subsubsection{Objects for picking}
The objects that we have used for conveyor tracking include a trapezoid, pipe socket, and circle ball.\\
\subsubsection{Socket-pipe} The socket pipe is: \\\\
\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=3cm]{Thesis-report/Figures/pipe.png} % Centered image
    \vspace{1cm} % Space after the image
\end{center} \\\\


\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=12cm]{Thesis-report/Figures/pipe_stl.jpg} % Centered image
    \vspace{1cm} % Space after the image
\end{center} \\\\
The figure above shows the STL file of the pipe, which is used to pick up the object.\\\\
\newpage
\subsubsection{Trapezoid}
The trapezoid  is: \\\\

\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=5cm]{Thesis-report/Figures/trapezoid.png} % Centered image
    \vspace{1cm} % Space after the image
\end{center}\\\\

\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=12cm]{Thesis-report/Figures/trapezoid_stl.jpg} % Centered image
    \vspace{1cm} % Space after the image
\end{center} \\\\
The above figure shows the STL file of the trapezoid,hich is used to pick up the object.\\\\


\subsubsection{Conveyor Belt}
The conveyor belt is the main belt where we have to place the object, thereby putting the object into dynamic mode.\\


\label{Literature}
\newpage
\subsection{Photoneo Camera-Web Interface}\\
The web interface shows the image of the object being detected; initially, we load the STL file of the object to be captured by the robot. The red color indicates the object, and once the object turns green, the object coordinate will be indicated as quaternion values. After getting the quaternion values, these are transferred to the Robotic controller which helps the robot to get into the desired target position.\\\\
\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=12cm]{Thesis-report/Figures/web interface.jpeg} % Centered image
    \vspace{1cm} % Space after the image
\end{center}\\\\


\begin{center}
    \vspace{-1cm} % Adjust this value if you want to move the image up or down
    \includegraphics[width=16cm]{Thesis-report/Figures/new.jpeg} % Centered image
    \vspace{1cm} % Space after the image
\end{center}\\\\
 The above figure represents the web interface that includes the camera triggering the object placed in the workspace. The green section represents the area for the workspace, and the red colored is the object to be picked up by the Robot.
\newpage

\chapter{\section{\mbox{Results and Discussions }}{{\normalfont\fontsize{14}{16}\bfseries}}
\label{Literature}

\newpage

\chapter{\section{\mbox{Conclusions and Future Prospects}}{{\normalfont\fontsize{14}{16}\bfseries}}
\label{Literature}
\subsection{Conclusion}


Conveyor tracking systems have become indispensable components in modern industrial and logistical operations, facilitating the seamless movement of goods across various stages of production, warehousing, and distribution. Over the years, advancements in technology have significantly enhanced the efficiency, accuracy, and reliability of these systems. Key achievements in conveyor tracking include:

\begin{itemize}

\item Enhanced Automation and Integration: The integration of conveyor tracking with other automated systems, such as Warehouse Management Systems (WMS) and Enterprise Resource Planning (ERP) software, has streamlined operations, reduced manual intervention, and minimized errors.

 \item Real-Time Monitoring and Data Analytics: Modern conveyor tracking systems leverage sensors, IoT devices, and advanced analytics to provide real-time insights into the movement of goods. This capability enables proactive decision-making, predictive maintenance, and optimized resource allocation.

\item Improved Accuracy and Efficiency: Innovations in tracking technologies, including barcode scanners, RFID tags, and machine vision, have significantly improved the accuracy of item identification and tracking, leading to reduced loss, theft, and misplacement of goods.

\item Scalability and Flexibility: Contemporary conveyor systems are designed to be highly scalable and adaptable, allowing businesses to adjust their operations based on fluctuating demand and changing market conditions without substantial infrastructure overhauls.

\item Sustainability and Energy Efficiency: Advances in conveyor technology have also focused on energy efficiency and sustainability, incorporating energy-saving motors, regenerative braking systems, and eco-friendly materials to reduce the environmental footprint of conveyor operations.
\end{itemize}


\newpage

\subsubsection{Future Scope }

The future of conveyor tracking is poised to be shaped by continued technological innovations and evolving industry demands. Potential developments and areas for growth include:

\begin{itemize}

\item Artificial Intelligence and Machine Learning: Integrating AI and ML algorithms can enhance predictive maintenance, optimize routing, and improve overall system intelligence. These technologies can analyze vast amounts of data to identify patterns, predict failures, and recommend operational adjustments in real-time.

\item Advanced IoT Integration: The proliferation of IoT devices will further enhance the connectivity and interoperability of conveyor tracking systems. Enhanced IoT integration will enable more granular monitoring, improved data collection, and seamless communication between disparate systems and devices.

\item Augmented Reality (AR) and Virtual Reality (VR): AR and VR technologies can revolutionize training, maintenance, and operational planning. For instance, maintenance personnel can use AR glasses to receive real-time guidance and overlays while servicing conveyor systems, reducing downtime and improving efficiency.

\item Blockchain for Enhanced Security and Transparency: Implementing blockchain technology can provide immutable records of product movement and handling, enhancing traceability, security, and transparency throughout the supply chain.

\item Robotic Integration and Automation: The integration of robotics with conveyor tracking systems can further automate material handling processes. Collaborative robots (cobots) can work alongside conveyor systems to perform tasks such as sorting, packaging, and inspection with greater precision and speed.

\item Sustainability and Green Technologies: Future conveyor systems will increasingly incorporate sustainable practices, such as the use of renewable energy sources, recyclable materials, and energy-efficient designs. Innovations aimed at reducing waste and minimizing the environmental impact will become paramount.

\item Enhanced User Interfaces and Human-Machine Interaction: The development of more intuitive and user-friendly interfaces will improve the interaction between operators and conveyor tracking systems. Voice-controlled interfaces, touchless controls, and advanced visualization tools will make system management more accessible and efficient.

\item Edge Computing and Enhanced Processing Capabilities: Implementing edge computing will allow data processing to occur closer to the source, reducing latency and improving the responsiveness of conveyor tracking systems. This will be particularly beneficial for real-time applications and environments with limited connectivity.

\item Customization and Modular Designs: Future conveyor tracking systems will offer greater customization and modularity, allowing businesses to tailor systems to their specific needs without extensive redesigns. Modular components can be easily added, removed, or reconfigured to adapt to changing operational requirements.

\item Enhanced Cybersecurity Measures: As conveyor tracking systems become more connected and reliant on digital technologies, ensuring robust cybersecurity will be critical. Future developments will focus on implementing advanced security protocols, encryption methods, and intrusion detection systems to protect against cyber threats.
\end{itemize}

\newpage

\chapter{\section{\mbox{References}}{{\normalfont\fontsize{14}{16}\bfseries}}
\label{Literature}



\end{document}


\addcontentsline{toc}{section}{Abstract} 


\pagenumbering{arabic}
\setcounter{page}{1}
\raggedright
\input{Chapters/Introduction}
\newpage
\input{Chapters/Literature Review }
\newpage
\input{Chapters/Methodology}
\newpage
\input{Chapters/Result}
\newpage
\input{Chapters/Conclusion and Future Prospects}
\newpage
\addcontentsline{toc}{section}{References}
\input{Chapters/Reference}
\newpage
\input{Chapters/Annex}





 

























